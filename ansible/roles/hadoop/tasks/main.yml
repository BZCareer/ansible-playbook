---
########################################################
# tasks setting up hadoop (only supports single node). I'm working on making it work across cluster.
# Author: Zak Hassan
# Usage: ansible-playbook site.yml or make install
#
#
# Description: Playbook created to demonstrate provisioning cloud environment in like less than 1 min. :)
#####################################################

- group: name={{ hadoop_gid }} state=present
- user: name={{ hadoop_uid }} comment="hadoop" group={{ hadoop_gid }} shell=/bin/bash

- name: stat /tmp/{{ hadoop_binary }}
  stat: path=/tmp/{{ hadoop_binary }}
  register: hdbinary_stat
- name: copy kafka to tmp folder
  copy: src={{ hadoop_binary }} dest=/tmp/ group={{ hadoop_gid }}
  when: hdbinary_stat.stat.exists == False
  tags:
    - provision_copy_to_remote

# #Uncomment the below section if you would like to download hadoop from some mirror.
# - name: download hadoop from apache mirror
#   command: wget http://apache.sunsite.ualberta.ca/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz -P /tmp/
#   tags:
#     - binary_download
- name: unzip tarball and place in /usr/local/
  command: tar zxvf /tmp/{{ hadoop_binary }}  -C {{ hadoop_prefix_dir }}
  tags:
    - provision_hdfs
- name: stat {{ hadoop_install_dir }}
  stat: path={{ hadoop_install_dir }}
  register: hd_dir_stat
- name: rename file
  command: creates="{{ hadoop_install_dir }}" mv {{ hadoop_prefix_dir }}/{{ hadoop_version }}/  {{ hadoop_install_dir }}
  when: hd_dir_stat.stat.exists == False
- name: change permissions to {{ hadoop_uid }} user with {{ hadoop_gid }} group for {{ hadoop_version }}
  command: chown -R {{ hadoop_uid }}:{{ hadoop_gid }} {{ hadoop_install_dir }}
# TODO: Need to use with_items here ...
- name: setup core-site.xml file
  template: src={{ item.name }}.j2
        dest=/usr/local/hadoop/etc/hadoop/{{ item.name }}
  with_items:
    - { name: core-site.xml   }
    - { name: hdfs-site.xml   }
    - { name: mapred-site.xml }
    - { name: yarn-site.xml   }

# TODO: Need to create http://docs.ansible.com/ansible/authorized_key_module.html
# So hadoop can ssh into alternate servers using hadoop acount
- lineinfile: dest=/home/hadoop/.bashrc regexp="HADOOP_HOME=" line="export HADOOP_HOME=/usr/local/hadoop"
- lineinfile: dest=/home/hadoop/.bashrc regexp="JAVA_HOME=" line="export JAVA_HOME=/usr/lib/jvm/java-1.7.0"
- lineinfile: dest=/home/hadoop/.bashrc regexp="PATH=" line="export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$JAVA_HOME/bin"
# Create systemd files

- name: copying slaves into conf location
  template: src=slaves dest=/usr/local/hadoop/etc/hadoop/slaves owner={{ hadoop_uid }} group={{ hadoop_gid }}
  when: api_hostname == "hadoopmaster"
- name: setup systemd unit file
  template: src={{ item.name }}.j2 dest=/lib/systemd/system/{{ item.name }}
  with_items:
    - { name: hadoop-dfs.service }
    - { name: hadoop-yarn.service }
  notify: reload systemd

# export HADOOP_PREFIX=/usr/local/hadoop
# export PATH=/usr/local/hadoop/bin:/usr/local/hadoop/sbin:$PATH
# TODO: hdfs namenode -format & $HADOOP_PREFIX/sbin/start-dfs.sh &  $HADOOP_PREFIX/sbin/start-yarn.sh
